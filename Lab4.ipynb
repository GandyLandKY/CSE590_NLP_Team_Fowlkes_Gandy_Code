{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5cc298",
   "metadata": {
    "colab_type": "text",
    "id": "C7oVbe_pLr3C"
   },
   "source": [
    "# Assignment 4 - Named Entity Recognition (NER)\n",
    "\n",
    "Welcome to the forth programming assignment of Course. In this assignment, you will learn to build more complicated models with pytorch. By completing this assignment, you will be able to: \n",
    "\n",
    "- Design the architecture of a neural network, train it, and test it. \n",
    "- Process features and represents them\n",
    "- Understand word padding\n",
    "- Implement LSTMs\n",
    "- Test with your own sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3572b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15da4b3a",
   "metadata": {
    "colab_type": "text",
    "id": "ftT-5-yynCtl"
   },
   "source": [
    "<a name=\"0\"></a>\n",
    "## Introduction\n",
    "\n",
    "We first start by defining named entity recognition (NER). NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. \n",
    "\n",
    "For example:\n",
    "\n",
    "<img src = 'images/ner.png' width=\"width\" height=\"height\" style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "Is labeled as follows: \n",
    "\n",
    "- French: geopolitical entity\n",
    "- Morocco: geographic entity \n",
    "- Christmas: time indicator\n",
    "\n",
    "Everything else that is labeled with an `O` is not considered to be a named entity. In this assignment, you will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. Then, you will load in the exact version of your model, which was trained for a longer period of time. You could then evaluate the trained version of your model to get 96% accuracy! Finally, you will be able to test your named entity recognition system with your own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283f1c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "JEY_jlQQR9SP",
    "outputId": "825f37fd-cf03-483a-a6b1-3d70da6f70f1"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from  torch.utils.data import DataLoader\n",
    "\n",
    "from utils import get_params, get_vocab\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "rnd.seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eddd16c",
   "metadata": {
    "colab_type": "text",
    "id": "_PpjG5MuLr3F"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Exploring the Data\n",
    "\n",
    "We will be using a dataset from Kaggle, which we will preprocess for you. The original data consists of four columns: the sentence number, the word, the part of speech of the word, and the tags.  A few tags you might expect to see are: \n",
    "\n",
    "* geo: geographical entity\n",
    "* org: organization\n",
    "* per: person \n",
    "* gpe: geopolitical entity\n",
    "* tim: time indicator\n",
    "* art: artifact\n",
    "* eve: event\n",
    "* nat: natural phenomenon\n",
    "* O: filler word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c7027",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "-Jur1JnXnCtr",
    "outputId": "88584ab6-0a15-489b-db5b-eb474e129c4f"
   },
   "outputs": [],
   "source": [
    "# display original kaggle data\n",
    "data = pd.read_csv(\"data/ner_dataset.csv\", encoding = \"ISO-8859-1\") \n",
    "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "print('SENTENCE:', train_sents)\n",
    "print('SENTENCE LABEL:', train_labels)\n",
    "print('ORIGINAL DATA:\\n', data.head(5))\n",
    "del(data, train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b4f51",
   "metadata": {
    "colab_type": "text",
    "id": "xoH6yBWVfzTb"
   },
   "source": [
    "<a name=\"1-1\"></a>\n",
    "### 1.1 - Importing the Data\n",
    "\n",
    "In this part, we will import the preprocessed data and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822f74b",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UauHjIKHWC0N"
   },
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d2f84",
   "metadata": {
    "colab_type": "text",
    "id": "mcQi6EmWnCty"
   },
   "source": [
    "`vocab` is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a `<PAD>` token. \n",
    "\n",
    "When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic `<PAD>` token to fill all the empty spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d31b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sm2P8y7zNgdU",
    "outputId": "1f1d077a-7c57-42df-fb67-48dd00da39ca"
   },
   "outputs": [],
   "source": [
    "# vocab translates from a word to a unique number\n",
    "print('vocab[\"the\"]:', vocab[\"the\"])\n",
    "# Pad token\n",
    "print('padded token:', vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0aed7",
   "metadata": {
    "colab_type": "text",
    "id": "IY6BTBjunCt1"
   },
   "source": [
    "The `tag_map` is a dictionary that maps the tags that you could have to numbers. Run the cell below to see the possible classes you will be predicting. The prepositions in the tags mean:\n",
    "* I: Token is inside an entity.\n",
    "* B: Token begins an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473d2a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZzMamaPcQXWP",
    "outputId": "4f04364c-a88c-4e77-f8bb-9bfcb422d5e9"
   },
   "outputs": [],
   "source": [
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820ad11",
   "metadata": {
    "colab_type": "text",
    "id": "3F1sUP_MnCt5"
   },
   "source": [
    "If you had the sentence \n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "The tags would look like:\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "where you would have three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon's last name to the sentence:\n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19168f7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "xM9B_Rwxd01i",
    "outputId": "db098ed6-4351-41f7-cfdb-e45dd3798ebf"
   },
   "outputs": [],
   "source": [
    "# Exploring information about the data\n",
    "print('The number of outputs is tag_map', len(tag_map))\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "g_vocab_size = len(vocab)\n",
    "print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
    "print('The training size is', t_size)\n",
    "print('The validation size is', v_size)\n",
    "print('An example of the first sentence is', t_sentences[0])\n",
    "print('An example of its corresponding label is', t_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1bf24f",
   "metadata": {
    "colab_type": "text",
    "id": "IPd5a-4_nCt8"
   },
   "source": [
    "So you can see that we have already encoded each sentence into a tensor by converting it into a number. We also have 16 possible tags (excluding the '0' tag), as shown in the tag map.\n",
    "\n",
    "\n",
    "<a name=\"1-2\"></a>\n",
    "### 1.2 - Data Generator\n",
    "\n",
    "In python, a generator is a function that behaves like an iterator. It returns the next item in a pre-defined sequence. Here is a [link](https://wiki.python.org/moin/Generators) to review python generators. \n",
    "\n",
    "In many AI applications it is very useful to have a data generator. You will now implement a data generator for our NER application.\n",
    "\n",
    "<a name=\"ex-1\"></a>\n",
    "### Exercise 1 - data_generator\n",
    "\n",
    "**Instructions:** Implement a dataset class that takes in ` x, y, pad ` where $x$ is a large list of sentences, and $y$ is a list of the tags associated with those sentences and pad is a pad value. This class you need to implement \n",
    "\n",
    "`__init__ function to initiate the sentences, labels and mask for the entire dataset`\n",
    "\n",
    "`__getitem__ function will take in index and return the coresponding sentence , label , mask`\n",
    "\n",
    "`__len__ return the size of the dataset`\n",
    " \n",
    "\n",
    "`X` and `Y` are arrays of dimension (`batch_size, max_len`), where `max_len` is the length of the longest sentence *in that batch*. You will pad the `X` and `Y` examples with the pad argument. If `shuffle=True`, the data will be traversed in a random order.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "Use this code as an outer loop\n",
    "```\n",
    "while True:  \n",
    "...  \n",
    "yield((X,Y))  \n",
    "```\n",
    "\n",
    "so your data generator runs continuously. Within that loop, define two `for` loops:  \n",
    "\n",
    "1. The first stores temporal lists of the data samples to be included in the batch, and finds the maximum length of the sentences contained in it.\n",
    "\n",
    "2. The second one moves the elements from the temporal list into NumPy arrays pre-filled with pad values.\n",
    "\n",
    "There are three features useful for defining this generator:\n",
    "\n",
    "1. The NumPy `full` function to fill the NumPy arrays with a pad value. See [full function documentation](https://numpy.org/doc/1.18/reference/generated/numpy.full.html).\n",
    "\n",
    "2. Tracking the current location in the incoming lists of sentences. Generators variables hold their values between invocations, so we create an `index` variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the `index` to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.  \n",
    "\n",
    "3. Since `batch_size` and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the `index` to 0. We can re-shuffle the list of indexes to produce different batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e115e1-8696-44c7-82e7-9b356986be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataSet(Dataset):\n",
    "    def __init__(self,x,y,pad):\n",
    "        \n",
    "        self._dataset_size = len(x)\n",
    "        max_length = 0\n",
    "\n",
    "        # create empty buffer for sentences and labels simply use empty array\n",
    "        buffer_x = []\n",
    "        buffer_y = []\n",
    "        \n",
    "        for sentence,label in zip(x,y):\n",
    "            max_length = max(max_length,len(sentence))\n",
    "            # add the sentence to the buffer\n",
    "            buffer_x.append(sentence)\n",
    "            # add the label to the buffer\n",
    "            buffer_y.append(label)\n",
    "\n",
    "        \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        \n",
    "        # create X,Y, NumPy arrays of size (dataset_size, max_len) 'full' of pad value\n",
    "        # use self._dataset_size , calculated max_length and fill the value with pad\n",
    "        X = None\n",
    "        # use self._dataset_size , calculated max_length and fill the value with pad\n",
    "        Y = None\n",
    "        # use self._dataset_size , calculated max_length and fill the value with 0\n",
    "        MASK = None\n",
    "\n",
    "        # copy values from lists to NumPy arrays. Use the buffered values\n",
    "        for i in range(self._dataset_size):\n",
    "            \n",
    "            # get the example (sentence as a tensor)\n",
    "            # in `buffer_x` at the `i` index\n",
    "            x_i = None\n",
    "            \n",
    "            # get the example (label as a tensor)\n",
    "            # in `buffer_y` at the `i` index\n",
    "            y_i = None\n",
    "            \n",
    "            # Walk through each sentence and words in x_i\n",
    "            for j in range(len(x_i)):\n",
    "                \n",
    "                # store the word in x_i at position j into X\n",
    "                X[i,j] = None\n",
    "                \n",
    "                # store the label in y_i at position j into Y\n",
    "                Y[i,j] = None\n",
    "\n",
    "                MASK[i,j] = 1\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        self.sentences = X\n",
    "        self.labels = Y\n",
    "        self.mask = MASK\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._dataset_size\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # return the reletive sentence (sentences are stored in self.sentences) data according to index\n",
    "        sentence = None\n",
    "        # return the reletive labels (sentences are stored in self.labels) data according to index\n",
    "        label = None\n",
    "        # return the reletive mask (sentences are stored in self.mask) data according to index\n",
    "        mask = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return {'sentence':sentence ,  'label': label, 'mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19cd9b8-09b8-49a0-a254-589f1c346aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,drop_last=True, device=\"cpu\"):\n",
    "    # load dataset as dataloader\n",
    "    dataloader = DataLoader(dataset = dataset, batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        batch = {}\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # implement a loop that iterate over data_dict.items() and return the related data according to the dictionary you return in the __getitem__ in dataset\n",
    "        # you need to copy the current batch in to the batch dictionary\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        yield batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6b994-016b-4102-bf3c-cd60e7a0da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "mini_sentences = t_sentences[0: 8]\n",
    "mini_labels = t_labels[0: 8]\n",
    "mini_dataset = NerDataSet(mini_sentences,mini_labels,vocab['<PAD>'])\n",
    "dg = generate_batches(mini_dataset,batch_size,False,False,\"cpu\")\n",
    "\n",
    "batch1 = next(dg)\n",
    "batch2 = next(dg)\n",
    "X1 , Y1, mask1 = batch1['sentence'], batch1['label'], batch1['mask']\n",
    "X2 , Y2, mask2 = batch2['sentence'], batch2['label'], batch1['mask']\n",
    "print(Y1.size(), X1.size(), Y2.size(), X2.size())\n",
    "print(X1[0][:], \"\\n\", Y1[0][:] , \"\\n\", mask1[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee3de7",
   "metadata": {
    "colab_type": "text",
    "id": "W-qWOhFunCuH"
   },
   "source": [
    "**Expected output:**   \n",
    "```\n",
    "torch.Size([5, 30]) torch.Size([5, 30]) torch.Size([3, 30]) torch.Size([3, 30])\n",
    "\n",
    "(5, 30) (5, 30) (5, 30) (5, 30)\n",
    "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
    "    12    13    14     9    15     1    16    17    18    19    20    21\n",
    " 35180 35180 35180 35180 35180 35180] \n",
    " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
    "     1     0     0     0     0     0     2     0     0     0     0     0\n",
    " 35180 35180 35180 35180 35180 35180]\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        0, 0, 0, 0, 0, 0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd14d5",
   "metadata": {
    "colab_type": "text",
    "id": "4SWxKhkVLr3P"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Building the Model\n",
    "\n",
    "You will now implement the model that will be able to determining the tags of sentences like the following:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "<img src = 'images/ner1.png' width=\"width\" height=\"height\" style=\"width:500px;height:150px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The model architecture will be as follows: \n",
    "\n",
    "<img src = 'images/ner2.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>\n",
    "\n",
    "\n",
    "Concretely, your inputs will be sentences represented as tensors that are fed to a model with:\n",
    "\n",
    "* An Embedding layer,\n",
    "* A LSTM layer\n",
    "* A Dense layer\n",
    "* A log softmax layer.\n",
    "\n",
    "Good news! We won't make you implement the LSTM cell drawn above. You will be in charge of the overall architecture of the model.\n",
    "\n",
    "<a name=\"ex-2\"></a>\n",
    "### Exercise 2 - NER\n",
    "\n",
    "**Instructions:** Implement the initialization step and the forward function of your Named Entity Recognition system.  \n",
    "Please utilize help function e.g. `help(tl.Dense)` for more information on a layer\n",
    "   \n",
    "\n",
    "\n",
    "-  [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html): Initializes the embedding. In this case it is the dimension of the model by the size of the vocabulary. \n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
    "    \n",
    "\n",
    "-  [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.ao.nn.quantizable.LSTM.html#lstm):`NN` LSTM layer. \n",
    "    - `LSTM(n_units)` Builds an LSTM layer with hidden state and cell sizes equal to `n_units`. In trax, `n_units` should be equal to the size of the embeddings `d_feature`.\n",
    "\n",
    "\n",
    "\n",
    "-  [nn.Linear](https://pytorch.org/docs/stable/generated/torch.ao.nn.quantized.functional.linear.html#linear):  A dense layer.\n",
    "    - `nn.Linear(input , output)`: The parameter `n_units` is the number of units chosen for this dense layer.  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839c55b-49e1-426b-b5ac-1d480f8fc6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER(nn.Module):\n",
    "    def __init__(self,tags,vocab_size,model_dim,hidden_size):\n",
    "        super(NER, self).__init__()\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        self.emb = None # Embedding layer\n",
    "        self.lstm = None # LSTM layer\n",
    "        self.linear = None # Linear layer with len(tags) units\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "\n",
    "    def forward(self,sentences):\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        embeds = None # send sentences to the emb layer\n",
    "        \n",
    "        lstm_out, _ = None # send embeds to the LSTM and get the output \n",
    "        \n",
    "        tag_space = None # send lstm_out to linear layer\n",
    "        \n",
    "        return tag_space\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b6fa8-6140-4ef8-9343-9a6dc89f63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing your model\n",
    "model = NER(tags=tag_map,vocab_size=len(vocab), model_dim=100,hidden_size=34)\n",
    "# display your model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c721c5",
   "metadata": {
    "colab_type": "text",
    "id": "p636VCSanCuS"
   },
   "source": [
    "**Expected output:**  \n",
    "```\n",
    "NER(\n",
    "  (emb): Embedding(35181, 50)\n",
    "  (lstm): LSTM(50, 17)\n",
    "  (linear): Linear(in_features=17, out_features=17, bias=Trone)\n",
    ")\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb2ccb",
   "metadata": {
    "colab_type": "text",
    "id": "4LkjXxxhLr3Z"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Train the Model \n",
    "\n",
    "This section will train your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafbd6c-a2f0-4966-87ea-1ebfd8e105b9",
   "metadata": {},
   "source": [
    "Use this step to implement the prediction \n",
    "\n",
    "\n",
    "we have the baches as represented:\n",
    "\n",
    "<img src = 'images/ner3.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>\n",
    "\n",
    "\n",
    "we need to flatten the matrix in order to be able to compare first we need to flat the mask and then use the mask to remove the padding as they are not part of our prediction\n",
    "\n",
    "<img src = 'images/ner4.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798a79a-1011-457a-8f76-dda70fed0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset , eval_dataset,train_step = 1, batch_size = 64 , verbos = False , save_model = False):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_loss, eval_loss = [], []\n",
    "    \n",
    "    for epoch in range(train_step):\n",
    "        \n",
    "        train_generator = generate_batches(train_dataset,batch_size=batch_size,shuffle=True,drop_last=False)\n",
    "        eval_generator = generate_batches(eval_dataset,batch_size=batch_size,shuffle=True,drop_last=False)\n",
    "\n",
    "        # train\n",
    "        train_runnign_loss = 0.0\n",
    "        train_runnign_acc = 0.0\n",
    "        model.train()\n",
    "        for batch_index, batch_dict in enumerate(train_generator):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "            # run the model and send the sentencs from the batch_dict\n",
    "            y_pred = model(None)\n",
    "\n",
    "            # get the labels from batch_dict\n",
    "            labels = None\n",
    "            \n",
    "            # make the pad to zero then we can remove the larg number that is not in the map\n",
    "            # to do that simply use mask > 0\n",
    "            mask_batch = None\n",
    "\n",
    "            # to make the pridection and compare the result we need to reshape the mask usign view function and send -1 dimension\n",
    "            # use view(-1) to flatten the mask_batch\n",
    "            mask_batch = None\n",
    "\n",
    "            # use view(-1,len(tag_map)) to flatten the y_pred make sure you are using the mask_batch to remove the padding\n",
    "            batch_y_pred = None\n",
    "\n",
    "            # use view(-1) to flatten the labels make sure you are using the mask_batch to remove the padding\n",
    "            batch_y_lables = None\n",
    "            \n",
    "            loss = loss_func(batch_y_pred,batch_y_lables.long())\n",
    "            \n",
    "            loss_batch = loss.item()\n",
    "            \n",
    "            train_runnign_loss += (loss_batch - train_runnign_loss) / (batch_index + 1)\n",
    "\n",
    "            # use the accuracy_score function and calculate the accuracy\n",
    "            # to be able to use the batch_y_pred make sure you detach and convert numpy batch_y_pred.detach().numpy()\n",
    "            # !!!! hint you should use np.argmax for batch_y_pred !!!\n",
    "            acc =  accuracy_score(None)\n",
    "            \n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            train_runnign_acc += (acc - train_runnign_acc) / (batch_index+1)\n",
    "\n",
    "            if epoch == 0 or epoch == train_step // 2 or epoch == train_step-1:\n",
    "                if verbos:\n",
    "                    print( \"(Prediction,Real) wrong Predictions : \\n \" , [(p,r) for p,r in zip( np.argmax( batch_y_pred.detach().numpy() , axis=1), batch_y_lables.numpy()) if p!=r ] , \"\\n accuracy: \\n\" , acc)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        #evaluation\n",
    "        eval_running_loss = 0.0\n",
    "        eval_running_acc = 0.0\n",
    "        model.eval()\n",
    "        for batch_index, batch_dict in enumerate(eval_generator):\n",
    "    \n",
    "            ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "            # run the model and send the sentencs from the batch_dict\n",
    "            y_pred = model(None)\n",
    "\n",
    "            # get the labels from batch_dict\n",
    "            labels = None\n",
    "            \n",
    "            # make the pad to zero then we can remove the larg number that is not in the map\n",
    "            # to do that simply use mask > 0\n",
    "            mask_batch = None\n",
    "\n",
    "            # to make the pridection and compare the result we need to reshape the mask usign view function and send -1 dimension\n",
    "            # use view(-1) to flatten the mask_batch\n",
    "            mask_batch = None\n",
    "\n",
    "            # use view(-1,len(tag_map)) to flatten the y_pred make sure you are using the mask_batch to remove the padding\n",
    "            batch_y_pred = None\n",
    "\n",
    "            # use view(-1) to flatten the labels make sure you are using the mask_batch to remove the padding\n",
    "            batch_y_lables = None\n",
    "            \n",
    "            loss = loss_func(batch_y_pred,batch_y_lables.long())\n",
    "            loss_batch = loss.item()\n",
    "            \n",
    "            eval_running_loss += (loss_batch - eval_running_loss) / (batch_index + 1)\n",
    "\n",
    "            # use the accuracy_score function and calculate the accuracy\n",
    "            # to be able to use the batch_y_pred make sure you detach and convert numpy batch_y_pred.detach().numpy()\n",
    "            # !!!! hint you should use np.argmax for batch_y_pred !!!\n",
    "            acc =  accuracy_score(None)\n",
    "\n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            eval_running_acc += (acc - eval_running_acc) / (batch_index+1)\n",
    "            \n",
    "        train_loss.append(train_runnign_loss)\n",
    "        eval_loss.append(eval_running_loss)\n",
    "        \n",
    "        print('step %s train loss %.3f  acc %.2f' % (epoch+ 1 , train_runnign_loss, train_runnign_acc ) , end = '\\r')\n",
    "        if epoch == 0 or epoch == train_step // 2 or epoch == train_step-1:\n",
    "            print('step %s: train loss %.3f acc %.2f'% (epoch+1,train_runnign_loss , train_runnign_acc))\n",
    "            print('step %s: eval loss %.3f  acc %.2f'% (epoch+1,eval_running_loss, eval_running_acc))\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(),'model/model.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba2db0-da15-4b91-b1fa-268a1549c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NerDataSet(t_sentences[:2],t_labels[:2], vocab['<PAD>'])\n",
    "eval_dataset = NerDataSet(v_sentences[:2],v_labels[:2], vocab['<PAD>'])\n",
    "model = NER(tags=tag_map,vocab_size=len(vocab), model_dim=100,hidden_size=64)\n",
    "train_model(model,train_dataset,eval_dataset,20,2, verbos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29b9ed-3fb2-49be-a973-62ab7de7df55",
   "metadata": {},
   "source": [
    "**Expected output:**   \n",
    "\n",
    "```\n",
    "\n",
    "(Prediction,Real) wrong Predictions : \n",
    "  [(7, 0), (6, 0), (4, 0), (4, 0), (4, 0), (13, 0), (7, 0), (13, 0), (13, 0), (4, 0), (4, 0), (7, 0), (10, 0), (13, 0), (4, 0), (4, 0), (8, 0), (4, 0), (4, 3), (4, 0), (7, 0), (8, 0), (4, 0), (7, 0), (4, 0), (13, 0), (13, 0), (13, 0), (7, 0), (4, 0), (13, 0), (6, 0), (4, 0), (7, 0), (7, 0), (8, 0), (13, 1), (4, 0), (8, 0), (13, 0), (4, 0), (4, 0), (8, 1), (7, 0), (4, 0), (15, 0), (8, 0), (6, 0), (6, 2), (7, 0), (13, 0), (4, 0), (4, 0), (7, 0)] \n",
    " accuracy: \n",
    " 0.0\n",
    "step 1: train loss 2.825 acc 0.00\n",
    "step 1: eval loss 2.826  acc 0.00\n",
    "(Prediction,Real) wrong Predictions : \n",
    "  [(13, 0), (13, 0), (13, 1), (7, 0), (13, 0), (4, 0), (7, 0), (4, 0), (1, 0), (10, 0), (13, 0), (13, 0)] \n",
    " accuracy: \n",
    " 0.7777777777777778\n",
    "step 11: train loss 2.523 acc 0.78\n",
    "step 11: eval loss 2.729  acc 0.24\n",
    "(Prediction,Real) wrong Predictions : \n",
    "  [(4, 0), (1, 0), (13, 0)] \n",
    " accuracy: \n",
    " 0.9444444444444444\n",
    "step 20: train loss 2.141 acc 0.94\n",
    "step 20: eval los\n",
    "\n",
    "```s 2.590  acc 0.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa1472",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "VU-j8hs-nCue",
    "outputId": "fbbbda7d-b6dd-42e4-a4c6-58c0a6e349b6"
   },
   "outputs": [],
   "source": [
    "train_steps = 20            # In coursera we can only train 100 steps\n",
    "#!rm -f 'model/model.pkl.gz'  # Remove old model.pkl if it exists\n",
    "train_dataset = NerDataSet(t_sentences,t_labels, vocab['<PAD>'])\n",
    "eval_dataset = NerDataSet(v_sentences,v_labels, vocab['<PAD>'])\n",
    "\n",
    "ner_model = NER(tags=tag_map,vocab_size=len(vocab), model_dim=200, hidden_size=64)\n",
    "train_model(ner_model,train_dataset,eval_dataset,train_steps,128, verbos=False, save_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f265e0f",
   "metadata": {
    "colab_type": "text",
    "id": "p1QvV66ZLr3i"
   },
   "source": [
    "**Expected output (Approximately)**\n",
    "\n",
    "```\n",
    "...\n",
    "step 1: train loss 0.763 acc 0.84\n",
    "step 1: eval loss 0.437  acc 0.89\n",
    "step 11: train loss 0.145 acc 0.96\n",
    "step 11: eval loss 0.213  acc 0.94\n",
    "step 20: train loss 0.127 acc 0.96\n",
    "step 20: eval loss 0.223  acc 0.94\n",
    "...\n",
    "```\n",
    "This value may change between executions, but it must be around 90% of accuracy on train and validations sets, after 100 training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62184a4d",
   "metadata": {
    "colab_type": "text",
    "id": "lQTurbC0nCuh"
   },
   "source": [
    "We have trained the model longer, and we give you such a trained model. In that way, we ensure you can continue with the rest of the assignment even if you had some troubles up to here, and also we are sure that everybody will get the same outputs for the last example. However, you are free to try your model, as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38c32e",
   "metadata": {
    "colab_type": "text",
    "id": "c4r-gXOZLr3j"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Compute Accuracy\n",
    "\n",
    "You will now evaluate in the test set. Previously, you have seen the accuracy on the training set and the validation (noted as eval) set. You will now evaluate on your test set. To get a good evaluation, you will need to create a mask to avoid counting the padding tokens when computing the accuracy. \n",
    "\n",
    "<a name=\"ex-4\"></a>\n",
    "### Exercise 4 - evaluate_prediction\n",
    "\n",
    "**Instructions:** Write a program that takes in your model and uses it to evaluate on the test set. You should be able to get an accuracy of 95%.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e24f4f-edc4-4493-b13b-87f68adaa789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the dimension and parameters in the loading model you might get an error to load a model\n",
    "\n",
    "ner_model_loaded = NER(tags=tag_map,vocab_size=len(vocab), model_dim=200, hidden_size=64)\n",
    "ner_model_loaded.load_state_dict(torch.load('model/model.pkl.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e16a3-aec1-4f54-8a85-3473a4f2b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model,dataset,batch_size):\n",
    "    model.eval()\n",
    "    test_generator = generate_batches(dataset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    test_running_acc = 0.0\n",
    "    for batch_index, batch_dict in enumerate(test_generator):\n",
    "\n",
    "         ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # run the model and send the sentencs from the batch_dict\n",
    "        y_pred = model(None)\n",
    "\n",
    "        # get the labels from batch_dict\n",
    "        labels = None\n",
    "        \n",
    "        # make the pad to zero then we can remove the larg number that is not in the map\n",
    "        # to do that simply use mask > 0\n",
    "        mask_batch = None\n",
    "\n",
    "        # to make the pridection and compare the result we need to reshape the mask usign view function and send -1 dimension\n",
    "        # use view(-1) to flatten the mask_batch\n",
    "        mask_batch = None\n",
    "\n",
    "        # use view(-1,len(tag_map)) to flatten the y_pred make sure you are using the mask_batch to remove the padding\n",
    "        batch_y_pred = None\n",
    "\n",
    "        # use view(-1) to flatten the labels make sure you are using the mask_batch to remove the padding\n",
    "        batch_y_lables = None\n",
    "        \n",
    "        # use the accuracy_score function and calculate the accuracy\n",
    "        # to be able to use the batch_y_pred make sure you detach and convert numpy batch_y_pred.detach().numpy()\n",
    "        # !!!! hint you should use np.argmax for batch_y_pred !!!\n",
    "        acc =  accuracy_score(None)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        test_running_acc += (acc - test_running_acc) / (batch_index+1)\n",
    "    \n",
    "    print('test acc %.2f'% ( test_running_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d334b21-f92e-45d4-bbb0-8b1f0669d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = NerDataSet(test_sentences,test_labels, vocab['<PAD>'])\n",
    "predict(ner_model_loaded,test_dataset,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebaf2ea-0006-4785-945c-5a5d1847a3d1",
   "metadata": {},
   "source": [
    "** Expected Output **\n",
    "```\n",
    "test acc 0.94\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb87052-eeb5-45c6-b914-7e951add320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function you will be using to test your own sentence.\n",
    "def predict(sentence, model, vocab, tag_map):\n",
    "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "    batch_data = np.zeros((1, len(s)))\n",
    "    batch_data[0][:] = s\n",
    "    sentence = torch.tensor(np.array(batch_data)).long()\n",
    "    output = model(sentence)\n",
    "    outputs = np.argmax(output.detach().numpy(), axis=2)\n",
    "    labels = list(tag_map.keys())\n",
    "    pred = []\n",
    "    for i in range(len(outputs[0])):\n",
    "        idx = outputs[0][i] \n",
    "        pred_label = labels[idx]\n",
    "        pred.append(pred_label)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748834e-13b4-4ef2-842e-1effa3e7e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the output for the introduction example\n",
    "#sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
    "#sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
    "\n",
    "# New york times news:\n",
    "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldnâ€™t necessarily come\"\n",
    "predictions = predict(sentence, ner_model_loaded, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e324b6-9ff7-435f-bf00-bf2fdc9f003d",
   "metadata": {},
   "source": [
    "** Expected output **\n",
    "\n",
    "it should be somethign like this however there is no guarante that you get the same result \n",
    "```\n",
    "Peter B-per\n",
    "White B-org\n",
    "House I-org\n",
    "Sunday B-tim\n",
    "morning I-tim\n",
    "White B-org\n",
    "House I-org\n",
    "\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb0aa6-cc4f-45e9-b255-d5bcdbee6fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
