{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8acde2d",
   "metadata": {},
   "source": [
    "# Understanding and Implementing Byte-Pair Encoding (BPE)\n",
    "\n",
    "This tutorial will walk you through the fundamentals of BPE tokenization and implement a custom tokenizer class called `BPE`. \n",
    "\n",
    "## What is BPE?\n",
    "Byte-Pair Encoding is a data compression technique that was adapted for subword tokenization. It's used by many modern language models like GPT and BERT to break words into meaningful subword units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99262212",
   "metadata": {},
   "source": [
    "# 1. Understanding BPE Tokenization\n",
    "\n",
    "BPE works by:\n",
    "1. Starting with a vocabulary of individual characters\n",
    "2. Iteratively finding the most frequent pair of adjacent tokens\n",
    "3. Merging these pairs to create new tokens\n",
    "4. Repeating until a desired vocabulary size is reached\n",
    "\n",
    "Let's see this with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16c8502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary size: 10\n",
      "Vocabulary: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
      "\n",
      "Starting BPE process:\n",
      "\n",
      "Most frequent pair: ('l', 'o') (frequency: 3)\n",
      "Words after merge: ['lo w', 'lo w e s t', 'n e w e r', 'w i d e r', 'n e w', 'lo w']\n",
      "Vocabulary size: 11\n",
      "Current vocabulary: ['d', 'e', 'i', 'l', 'lo', 'n', 'o', 'r', 's', 't', 'w']\n",
      "\n",
      "Most frequent pair: ('lo', 'w') (frequency: 3)\n",
      "Words after merge: ['low', 'low e s t', 'n e w e r', 'w i d e r', 'n e w', 'low']\n",
      "Vocabulary size: 12\n",
      "Current vocabulary: ['d', 'e', 'i', 'l', 'lo', 'low', 'n', 'o', 'r', 's', 't', 'w']\n",
      "\n",
      "Most frequent pair: ('n', 'e') (frequency: 2)\n",
      "Words after merge: ['low', 'low e s t', 'ne w e r', 'w i d e r', 'ne w', 'low']\n",
      "Vocabulary size: 13\n",
      "Current vocabulary: ['d', 'e', 'i', 'l', 'lo', 'low', 'n', 'ne', 'o', 'r', 's', 't', 'w']\n",
      "\n",
      "Most frequent pair: ('ne', 'w') (frequency: 2)\n",
      "Words after merge: ['low', 'low e s t', 'new e r', 'w i d e r', 'new', 'low']\n",
      "Vocabulary size: 14\n",
      "Current vocabulary: ['d', 'e', 'i', 'l', 'lo', 'low', 'n', 'ne', 'new', 'o', 'r', 's', 't', 'w']\n",
      "\n",
      "Most frequent pair: ('e', 'r') (frequency: 2)\n",
      "Words after merge: ['low', 'low e s t', 'new er', 'w i d er', 'new', 'low']\n",
      "Vocabulary size: 15\n",
      "Current vocabulary: ['d', 'e', 'er', 'i', 'l', 'lo', 'low', 'n', 'ne', 'new', 'o', 'r', 's', 't', 'w']\n"
     ]
    }
   ],
   "source": [
    "# Example of BPE process\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def visualize_bpe_step(word_list, vocab_size=10):\n",
    "    # Count initial character frequencies\n",
    "    words = [' '.join(list(word)) for word in word_list]\n",
    "    \n",
    "    # Current vocabulary is individual characters\n",
    "    vocab = set(''.join(word_list))\n",
    "    print(f\"Initial vocabulary size: {len(vocab)}\")\n",
    "    print(\"Vocabulary:\", sorted(list(vocab)))\n",
    "    print(\"\\nStarting BPE process:\")\n",
    "    \n",
    "    while len(vocab) < vocab_size:\n",
    "        # Get pairs of tokens\n",
    "        pairs = Counter()\n",
    "        for word in words:\n",
    "            tokens = word.split()\n",
    "            for i in range(len(tokens)-1):\n",
    "                pair = (tokens[i], tokens[i+1])\n",
    "                pairs[pair] += 1\n",
    "        \n",
    "        if not pairs:\n",
    "            break\n",
    "            \n",
    "        # Get most frequent pair\n",
    "        best_pair = max(pairs.items(), key=lambda x: x[1])\n",
    "        print(f\"\\nMost frequent pair: {best_pair[0]} (frequency: {best_pair[1]})\")\n",
    "        \n",
    "        # Merge pair in all words\n",
    "        new_token = ''.join(best_pair[0])\n",
    "        vocab.add(new_token)\n",
    "        \n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = word.replace(' '.join(best_pair[0]), new_token)\n",
    "            new_words.append(new_word)\n",
    "        words = new_words\n",
    "        \n",
    "        print(f\"Words after merge: {words}\")\n",
    "        print(f\"Vocabulary size: {len(vocab)}\")\n",
    "        print(\"Current vocabulary:\", sorted(list(vocab)))\n",
    "\n",
    "# Example usage\n",
    "word_list = ['low', 'lowest', 'newer', 'wider', 'new', 'low']\n",
    "visualize_bpe_step(word_list, vocab_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e6c5e",
   "metadata": {},
   "source": [
    "# How to tokenize the courpus\n",
    "\n",
    "1. easy split based on space\n",
    "2. using regex to follow some rules and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1f0e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', ' world', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "GPT4PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "compiler = re.compile(GPT4PATTERN)\n",
    "re.findall(compiler, \"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b6213",
   "metadata": {},
   "source": [
    "# Understanding the Regex Pattern\n",
    "\n",
    "Let's break down the regex pattern used in our tokenizer. First, let's understand what `regex` is and why we use it:\n",
    "\n",
    "## What is the `regex` module?\n",
    "```python\n",
    "import regex as re\n",
    "```\n",
    "- `regex` is an enhanced version of Python's built-in `re` module\n",
    "- It adds support for Unicode properties (`\\p{L}`, `\\p{N}`, etc.)\n",
    "- It's more powerful than `re` for handling complex patterns\n",
    "\n",
    "## Breaking Down the Pattern\n",
    "```python\n",
    "GPT4PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```\n",
    "\n",
    "This pattern is broken into several parts, separated by `|` (OR operator):\n",
    "\n",
    "1. `'(?i:[sdmt]|ll|ve|re)`\n",
    "   - Matches contractions like 's, 'll, 've, 're\n",
    "   - `(?i:...)` makes this part case-insensitive\n",
    "   - Examples: \"I'm\", \"I'll\", \"I've\", \"they're\"\n",
    "\n",
    "2. `[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+`\n",
    "   - `\\p{L}` matches any kind of letter from any language\n",
    "   - `\\p{N}` matches any kind of numeric character\n",
    "   - `[^\\r\\n\\p{L}\\p{N}]?+` optionally matches one non-letter, non-number character\n",
    "   - Examples: \"Hello\", \"ä¸–ç•Œ\", \"cafÃ©\"\n",
    "\n",
    "3. `\\p{N}{1,3}`\n",
    "   - Matches 1 to 3 consecutive numbers\n",
    "   - Examples: \"1\", \"42\", \"999\"\n",
    "\n",
    "4. ` ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*`\n",
    "   - Matches punctuation and symbols\n",
    "   - ` ?` optionally matches a space\n",
    "   - `[^\\s\\p{L}\\p{N}]++` matches non-space, non-letter, non-number characters\n",
    "   - Examples: \"!\", \",\", \"?\"\n",
    "\n",
    "5. `\\s*[\\r\\n]|\\s+(?!\\S)|\\s+`\n",
    "   - Matches different types of whitespace\n",
    "   - `\\s*[\\r\\n]` matches newlines with optional spaces\n",
    "   - `\\s+(?!\\S)` matches spaces at the end of text\n",
    "   - `\\s+` matches any other whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd6d235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Simple text\n",
      "Text: Hello, world!\n",
      "Tokens: ['Hello', ',', ' world', '!']\n",
      "\n",
      "Breakdown:\n",
      "1. 'Hello'\n",
      "2. ','\n",
      "3. ' world'\n",
      "4. '!'\n",
      "\n",
      "Example 2: Contractions\n",
      "Text: I'm going to the store, they've been there.\n",
      "Tokens: ['I', \"'m\", ' going', ' to', ' the', ' store', ',', ' they', \"'ve\", ' been', ' there', '.']\n",
      "\n",
      "Breakdown:\n",
      "1. 'I'\n",
      "2. ''m'\n",
      "3. ' going'\n",
      "4. ' to'\n",
      "5. ' the'\n",
      "6. ' store'\n",
      "7. ','\n",
      "8. ' they'\n",
      "9. ''ve'\n",
      "10. ' been'\n",
      "11. ' there'\n",
      "12. '.'\n",
      "\n",
      "Example 3: Mixed content\n",
      "Text: In 2023, AI's progress is amazing! ðŸ˜Š\n",
      "Tokens: ['In', ' ', '202', '3', ',', ' AI', \"'s\", ' progress', ' is', ' amazing', '!', ' ðŸ˜Š']\n",
      "\n",
      "Breakdown:\n",
      "1. 'In'\n",
      "2. '[SPACE]'\n",
      "3. '202'\n",
      "4. '3'\n",
      "5. ','\n",
      "6. ' AI'\n",
      "7. ''s'\n",
      "8. ' progress'\n",
      "9. ' is'\n",
      "10. ' amazing'\n",
      "11. '!'\n",
      "12. ' ðŸ˜Š'\n"
     ]
    }
   ],
   "source": [
    "# Let's see how this pattern works with examples\n",
    "import regex as re\n",
    "\n",
    "def explain_matches(text):\n",
    "    \"\"\"Show how the pattern matches parts of the text\"\"\"\n",
    "    pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    compiler = re.compile(pattern)\n",
    "    matches = compiler.findall(text)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Tokens:\", matches)\n",
    "    print(\"\\nBreakdown:\")\n",
    "    for i, token in enumerate(matches, 1):\n",
    "        if token.isspace():\n",
    "            token_display = \"[SPACE]\"\n",
    "        else:\n",
    "            token_display = token\n",
    "        print(f\"{i}. '{token_display}'\")\n",
    "\n",
    "# Example 1: Simple text with punctuation\n",
    "print(\"Example 1: Simple text\")\n",
    "explain_matches(\"Hello, world!\")\n",
    "\n",
    "# Example 2: Text with contractions\n",
    "print(\"\\nExample 2: Contractions\")\n",
    "explain_matches(\"I'm going to the store, they've been there.\")\n",
    "\n",
    "# Example 3: Mixed content\n",
    "print(\"\\nExample 3: Mixed content\")\n",
    "explain_matches(\"In 2023, AI's progress is amazing! ðŸ˜Š\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726bf529",
   "metadata": {},
   "source": [
    "# Understanding Pattern Compilation\n",
    "\n",
    "When we use:\n",
    "```python\n",
    "compiler = re.compile(GPT4PATTERN)\n",
    "```\n",
    "\n",
    "This is important because:\n",
    "\n",
    "1. **Performance**: Compiling a pattern is more efficient when you plan to use it multiple times. The pattern is parsed once and can be reused.\n",
    "\n",
    "2. **Validation**: Compilation checks if the pattern is valid. If there's a syntax error, it will be caught at compile time rather than during execution.\n",
    "\n",
    "3. **Options**: You can set flags during compilation that affect how the pattern works (like case-insensitivity).\n",
    "\n",
    "## Pattern Execution Methods\n",
    "\n",
    "The `re` module provides several ways to use patterns:\n",
    "\n",
    "1. `findall()`: Returns all non-overlapping matches in a string\n",
    "   ```python\n",
    "   re.findall(compiler, \"Hello, world!\")\n",
    "   ```\n",
    "   - Returns a list of all matches\n",
    "   - Each match is returned as a string\n",
    "   - Great for tokenization where you want all pieces\n",
    "\n",
    "2. `match()`: Tries to match at the beginning of the string\n",
    "\n",
    "3. `search()`: Looks for a match anywhere in the string\n",
    "\n",
    "4. `split()`: Splits the string by the pattern\n",
    "\n",
    "For tokenization, we use `findall()` because we want to:\n",
    "- Get all tokens in the text\n",
    "- Preserve the order of tokens\n",
    "- Include both words and separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215bfbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. findall() results:\n",
      "['Hello', ',', ' world', '!', ' This', ' is', ' a', ' test', '.']\n",
      "\n",
      "2. match() result:\n",
      "Matched: 'Hello'\n",
      "Start: 0, End: 5\n",
      "\n",
      "3. search() result:\n",
      "Found: 'Hello'\n",
      "Start: 0, End: 5\n",
      "\n",
      "4. split() results:\n",
      "['', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "For tokenization, findall() is best because:\n",
      "- Gets all tokens in order\n",
      "- Preserves all parts of the text\n",
      "- Returns a simple list of strings\n"
     ]
    }
   ],
   "source": [
    "# Let's compare different regex methods\n",
    "import regex as re\n",
    "\n",
    "text = \"Hello, world! This is a test.\"\n",
    "pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "compiler = re.compile(pattern)\n",
    "\n",
    "# 1. findall() - Get all matches\n",
    "print(\"1. findall() results:\")\n",
    "findall_results = compiler.findall(text)\n",
    "print(findall_results)\n",
    "\n",
    "# 2. match() - Try to match at the beginning\n",
    "print(\"\\n2. match() result:\")\n",
    "match_result = compiler.match(text)\n",
    "if match_result:\n",
    "    print(f\"Matched: '{match_result.group()}'\")\n",
    "    print(f\"Start: {match_result.start()}, End: {match_result.end()}\")\n",
    "else:\n",
    "    print(\"No match at beginning\")\n",
    "\n",
    "# 3. search() - Find first match anywhere\n",
    "print(\"\\n3. search() result:\")\n",
    "search_result = compiler.search(text)\n",
    "if search_result:\n",
    "    print(f\"Found: '{search_result.group()}'\")\n",
    "    print(f\"Start: {search_result.start()}, End: {search_result.end()}\")\n",
    "else:\n",
    "    print(\"No match found\")\n",
    "\n",
    "# 4. split() - Split by pattern\n",
    "print(\"\\n4. split() results:\")\n",
    "split_results = compiler.split(text)\n",
    "print(split_results)\n",
    "\n",
    "# Show which method is best for tokenization\n",
    "print(\"\\nFor tokenization, findall() is best because:\")\n",
    "print(\"- Gets all tokens in order\")\n",
    "print(\"- Preserves all parts of the text\")\n",
    "print(\"- Returns a simple list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f42747c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[72, 101, 108, 108, 111], [44], [32, 119, 111, 114, 108, 100], [33]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "GPT4PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "compiler = re.compile(GPT4PATTERN)\n",
    "words = re.findall(compiler, \"Hello, world!\")\n",
    "ids = [list(ch.encode('utf-8')) for ch in words]\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e01c1",
   "metadata": {},
   "source": [
    "# Understanding and Implementing the BPE Class\n",
    "\n",
    "Let's break down how to implement a Byte-Pair Encoding (BPE) tokenizer step by step. This guide will help you understand each component and implement your own BPE tokenizer.\n",
    "\n",
    "## Class Structure Overview\n",
    "\n",
    "The BPE class needs three main components:\n",
    "1. Initialization and storage of basic information\n",
    "2. Methods to analyze and count token pairs\n",
    "3. Methods to merge tokens and update the vocabulary\n",
    "\n",
    "## Step-by-Step Implementation Guide\n",
    "\n",
    "### Step 1: Class Initialization\n",
    "Your class needs to track:\n",
    "- The target vocabulary size\n",
    "- The input texts\n",
    "- The current vocabulary\n",
    "- The merge operations performed\n",
    "\n",
    "Think about:\n",
    "- What data structures would be best for storing the vocabulary?\n",
    "- How will you keep track of merge operations?\n",
    "- What initial size should your vocabulary have?\n",
    "\n",
    "### Step 2: Statistics Collection (get_stats method)\n",
    "This method needs to:\n",
    "1. Look at pairs of adjacent tokens in the text\n",
    "2. Count how often each pair appears\n",
    "3. Return the counts for all pairs\n",
    "\n",
    "Consider:\n",
    "- How to efficiently count pairs\n",
    "- How to handle pairs that span word boundaries\n",
    "- What data structure is best for storing counts\n",
    "\n",
    "### Step 3: Pair Merging (merge_pair method)\n",
    "This method should:\n",
    "1. Take a pair of tokens and their new token ID\n",
    "2. Find all occurrences of this pair in the text\n",
    "3. Replace them with the new token ID\n",
    "\n",
    "Think about:\n",
    "- How to efficiently find pairs in the text\n",
    "- How to handle overlapping pairs\n",
    "- How to maintain the text structure while merging\n",
    "\n",
    "### Step 4: BPE Learning (learn_bpe method)\n",
    "This is the main method that:\n",
    "1. Initializes the base vocabulary (256 bytes)\n",
    "2. Processes the input text into initial tokens\n",
    "3. Repeatedly:\n",
    "   - Counts token pairs\n",
    "   - Finds the most frequent pair\n",
    "   - Merges this pair into a new token\n",
    "   - Updates the vocabulary\n",
    "   - Continues until reaching the target vocabulary size\n",
    "\n",
    "Consider:\n",
    "- How to handle text preprocessing\n",
    "- When to stop merging\n",
    "- How to maintain the vocabulary efficiently\n",
    "\n",
    "## Implementation Tips\n",
    "\n",
    "1. Data Structures:\n",
    "   - Use dictionaries for the vocabulary (fast lookup)\n",
    "   - Use defaultdict for counting pairs (convenient counting)\n",
    "   - Consider using lists for storing merge operations\n",
    "\n",
    "2. Text Processing:\n",
    "   - Start with byte-level tokenization\n",
    "   - Use regex for initial text splitting\n",
    "   - Handle UTF-8 encoding properly\n",
    "\n",
    "3. Optimization Tips:\n",
    "   - Cache frequently accessed data\n",
    "   - Avoid unnecessary string operations\n",
    "   - Use efficient data structures for frequent operations\n",
    "\n",
    "4. Error Handling:\n",
    "   - Check input parameters\n",
    "   - Handle edge cases (empty text, small vocab size)\n",
    "   - Validate vocabulary size (>= 256)\n",
    "\n",
    "## Common Pitfalls to Avoid\n",
    "\n",
    "1. Vocabulary Size:\n",
    "   - Don't forget the base vocabulary of 256 bytes\n",
    "   - Ensure target size is greater than base size\n",
    "\n",
    "2. Merging Process:\n",
    "   - Be careful with overlapping pairs\n",
    "   - Don't modify data while iterating\n",
    "   - Track indices carefully during merges\n",
    "\n",
    "3. Text Handling:\n",
    "   - Remember to handle UTF-8 properly\n",
    "   - Consider whitespace and special characters\n",
    "   - Handle case sensitivity correctly\n",
    "\n",
    "## Testing Your Implementation\n",
    "\n",
    "Test your implementation with:\n",
    "1. Simple repeated words\n",
    "2. Mixed case text\n",
    "3. Special characters\n",
    "4. Unicode text\n",
    "5. Various vocabulary sizes\n",
    "\n",
    "## Assignment Tips\n",
    "\n",
    "When implementing your own BPE:\n",
    "1. Start with the basic structure\n",
    "2. Implement one method at a time\n",
    "3. Test each method thoroughly\n",
    "4. Add features incrementally\n",
    "5. Document your code\n",
    "6. Add error handling last\n",
    "\n",
    "Remember: BPE is an iterative process. Make sure each step works correctly before moving to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class BPE():\n",
    "\n",
    "    def __init__(self,vocab_size,texts) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.texts = texts\n",
    "        self.vocab = dict()\n",
    "        self.bpe_merges = []\n",
    "\n",
    "    def get_stats(self, ids, pairs= None):\n",
    "        \"\"\"\n",
    "        Get counts of all pairs of adjacent symbols in the dataset\n",
    "        \"\"\"\n",
    "        pairs = defaultdict(int) if pairs is None else pairs\n",
    "        \n",
    "        # Code here to count pairs \n",
    "        # --------------------------------------------  \n",
    "\n",
    "        return pairs\n",
    "    \n",
    "    def merge_pair(self,ids, pair, idx):\n",
    "        \"\"\"\n",
    "        Merge all occurrences of the given pair in the dataset\n",
    "        \"\"\"\n",
    "        new_ids = []\n",
    "\n",
    "        #code here to merge pair\n",
    "        # ---------------------------------------\n",
    "        return new_ids\n",
    "    \n",
    "    def learn_bpe(self):\n",
    "        \"\"\"\n",
    "        Learn BPE merges until reaching the desired vocabulary size\n",
    "        \"\"\"\n",
    "        # Code here to Initialize vocabulary with individual characters\n",
    "        self.vocab = None\n",
    "\n",
    "        corpus = \"\" \n",
    "        for text in self.texts:\n",
    "            corpus += text\n",
    "\n",
    "        pattern = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        compiler = re.compile(pattern)\n",
    "        words = re.findall(compiler, corpus)\n",
    "\n",
    "        ids = [list(ch.encode('utf-8')) for ch in words]\n",
    "\n",
    "\n",
    "    \n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "        idx = len(self.vocab)\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pairs = defaultdict(int)\n",
    "            for chunk_ids in ids:\n",
    "                self.get_stats(chunk_ids,pairs)\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Code here to Get most frequent pair\n",
    "            best_pair = None\n",
    "\n",
    "\n",
    "            self.bpe_merges.append(best_pair)\n",
    "            new_token = self.vocab[best_pair[0]] + self.vocab[best_pair[1]]\n",
    "            self.vocab[idx] = new_token\n",
    "            \n",
    "            # Merge pair in all texts\n",
    "            new_ids = []\n",
    "            for chunk_ids in ids:\n",
    "                new_text = self.merge_pair(chunk_ids, best_pair, idx)\n",
    "                new_ids.append(new_text)\n",
    "            ids = new_ids\n",
    "            idx+=1\n",
    "        \n",
    "        print(\"vocab size:\", len(self.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10992cd",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2873be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_bpe_results(tokenizer, test_text, description):\n",
    "    \"\"\"\n",
    "    Validate the BPE tokenizer results for different test cases\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Validating {description} ===\")\n",
    "    print(f\"Test text: {test_text}\")\n",
    "    \n",
    "    # Check vocabulary fundamentals\n",
    "    vocab_size = len(tokenizer.vocab)\n",
    "    print(f\"Current vocab size: {vocab_size}\")\n",
    "    print(f\"Base vocabulary size (bytes): 256\")\n",
    "    print(f\"Additional merged tokens: {vocab_size - 256}\")\n",
    "    \n",
    "    # Analyze merge patterns\n",
    "    if tokenizer.bpe_merges:\n",
    "        print(\"\\nSample merges that affect this text:\")\n",
    "        relevant_merges = []\n",
    "        test_bytes = ''.join(test_text).encode('utf-8')\n",
    "        \n",
    "        for merge in tokenizer.bpe_merges:\n",
    "            merged = tokenizer.vocab[merge[0]] + tokenizer.vocab[merge[1]]\n",
    "            if merged in test_bytes:\n",
    "                relevant_merges.append(merged.decode('utf-8', errors='ignore'))\n",
    "                if len(relevant_merges) >= 5:  # Show up to 5 relevant merges\n",
    "                    break\n",
    "        \n",
    "        if relevant_merges:\n",
    "            for i, merge in enumerate(relevant_merges, 1):\n",
    "                print(f\"Relevant merge {i}: '{merge}'\")\n",
    "        else:\n",
    "            print(\"No directly relevant merges found for this text\")\n",
    "    \n",
    "    # Special case validations\n",
    "    print(\"\\nValidation checks:\")\n",
    "    \n",
    "    # Check case sensitivity\n",
    "    if any(word.isupper() for word in test_text.split()):\n",
    "        upper_preserved = any(any(c.isupper() for c in v.decode('utf-8', errors='ignore'))\n",
    "                            for v in tokenizer.vocab.values())\n",
    "        print(f\"Case sensitivity preserved: {'âœ“' if upper_preserved else 'âœ—'}\")\n",
    "    \n",
    "    # Check for common subwords\n",
    "    if \"play\" in test_text.lower():\n",
    "        play_merged = any(b\"play\" in v for v in tokenizer.vocab.values())\n",
    "        print(f\"Common prefix 'play' merged: {'âœ“' if play_merged else 'âœ—'}\")\n",
    "    \n",
    "    # Check for repetition handling\n",
    "    if len(set(test_text.split())) < len(test_text.split()):\n",
    "        repeated_words = [word for word in set(test_text.split()) \n",
    "                         if test_text.split().count(word) > 1]\n",
    "        for word in repeated_words[:3]:  # Check up to 3 repeated words\n",
    "            word_merged = any(word.encode('utf-8') in v for v in tokenizer.vocab.values())\n",
    "            print(f\"Repeated word '{word}' merged: {'âœ“' if word_merged else 'âœ—'}\")\n",
    "    \n",
    "    # Check for special character handling\n",
    "    if any(not c.isalnum() and not c.isspace() for c in test_text):\n",
    "        spec_chars_preserved = any(not all(c.isalnum() or c.isspace() \n",
    "                                         for c in v.decode('utf-8', errors='ignore'))\n",
    "                                 for v in tokenizer.vocab.values())\n",
    "        print(f\"Special characters preserved: {'âœ“' if spec_chars_preserved else 'âœ—'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de00e0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and training tokenizer...\n",
      "Initial vocabulary size: 256\n",
      "vocab size: 400\n",
      "\n",
      "Running validation tests...\n",
      "\n",
      "Test 1: Basic English\n",
      "\n",
      "=== Validating Basic English text ===\n",
      "Test text: The quick brown fox jumps over the lazy dog\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: 'er'\n",
      "Relevant merge 2: 'la'\n",
      "Relevant merge 3: ' t'\n",
      "Relevant merge 4: 'he'\n",
      "Relevant merge 5: 'um'\n",
      "\n",
      "Validation checks:\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 2: Repeated Words\n",
      "\n",
      "=== Validating Text with repeated words ===\n",
      "Test text: the the the quick quick fox fox dog dog\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: ' t'\n",
      "Relevant merge 2: 'he'\n",
      "Relevant merge 3: ' the'\n",
      "Relevant merge 4: 'ic'\n",
      "Relevant merge 5: ' d'\n",
      "\n",
      "Validation checks:\n",
      "Repeated word 'dog' merged: âœ“\n",
      "Repeated word 'the' merged: âœ“\n",
      "Repeated word 'quick' merged: âœ“\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 3: Mixed Case\n",
      "\n",
      "=== Validating Mixed case handling ===\n",
      "Test text: Hello WORLD wOrLd World HELLO\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: 'lo'\n",
      "Relevant merge 2: 'or'\n",
      "Relevant merge 3: ' w'\n",
      "Relevant merge 4: 'He'\n",
      "Relevant merge 5: 'Hel'\n",
      "\n",
      "Validation checks:\n",
      "Case sensitivity preserved: âœ“\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 4: Numbers and Special Characters\n",
      "\n",
      "=== Validating Numbers and special characters ===\n",
      "Test text: User123 has $100.00 in their account!\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: 'er'\n",
      "Relevant merge 2: ' t'\n",
      "Relevant merge 3: ' a'\n",
      "Relevant merge 4: 'he'\n",
      "Relevant merge 5: 'ac'\n",
      "\n",
      "Validation checks:\n",
      "Special characters preserved: âœ“\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 5: Subwords\n",
      "\n",
      "=== Validating Common subword patterns ===\n",
      "Test text: playing player played plays playful playground\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: 'er'\n",
      "Relevant merge 2: 'la'\n",
      "Relevant merge 3: 'lay'\n",
      "Relevant merge 4: ' p'\n",
      "Relevant merge 5: ' play'\n",
      "\n",
      "Validation checks:\n",
      "Common prefix 'play' merged: âœ“\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 6: Contractions\n",
      "\n",
      "=== Validating Contractions ===\n",
      "Test text: I'm don't won't can't they've you're\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: ' t'\n",
      "Relevant merge 2: ' c'\n",
      "Relevant merge 3: 'he'\n",
      "Relevant merge 4: 'on'\n",
      "Relevant merge 5: ' the'\n",
      "\n",
      "Validation checks:\n",
      "Special characters preserved: âœ“\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 7: Whitespace\n",
      "\n",
      "=== Validating Whitespace handling ===\n",
      "Test text: Multiple    spaces   and   tabs   here\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: '  '\n",
      "Relevant merge 2: 'er'\n",
      "Relevant merge 3: ' t'\n",
      "Relevant merge 4: ' a'\n",
      "Relevant merge 5: 'he'\n",
      "\n",
      "Validation checks:\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 8: Unicode\n",
      "\n",
      "=== Validating Unicode character handling ===\n",
      "Test text: cafÃ© rÃ©sumÃ© chÃ¢teau Å¡koda\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: ' c'\n",
      "Relevant merge 2: 'um'\n",
      "Relevant merge 3: 'Ã©'\n",
      "Relevant merge 4: 'od'\n",
      "\n",
      "Validation checks:\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 9: Punctuation\n",
      "\n",
      "=== Validating Punctuation handling ===\n",
      "Test text: Hello, world! How are you? This is: amazing.\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: ' a'\n",
      "Relevant merge 2: 'lo'\n",
      "Relevant merge 3: 'or'\n",
      "Relevant merge 4: 'is'\n",
      "Relevant merge 5: ' w'\n",
      "\n",
      "Validation checks:\n",
      "Special characters preserved: âœ“\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 10: Compound Words\n",
      "\n",
      "=== Validating Long compound words ===\n",
      "Test text: tokenization implementation methodology understanding\n",
      "Current vocab size: 400\n",
      "Base vocabulary size (bytes): 256\n",
      "Additional merged tokens: 144\n",
      "\n",
      "Sample merges that affect this text:\n",
      "Relevant merge 1: 'er'\n",
      "Relevant merge 2: 'nd'\n",
      "Relevant merge 3: 'on'\n",
      "Relevant merge 4: 'ers'\n",
      "Relevant merge 5: 'lo'\n",
      "\n",
      "Validation checks:\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a single tokenizer with comprehensive training data\n",
    "print(\"Creating and training tokenizer...\")\n",
    "training_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "Hello World! This is a test of the tokenizer.\n",
    "We have numbers like 123, 456, and 789.\n",
    "Playing player played plays playful playground\n",
    "UPPERCASE, lowercase, and MiXeDcAsE words.\n",
    "Special characters: !@#$%^&*()_+-=[]{}|;:',.<>?\n",
    "Multiple     spaces    and   tabs   are    here.\n",
    "Unicode characters: cafÃ© rÃ©sumÃ© chÃ¢teau Å¡koda\n",
    "Contractions: don't won't can't I'm you're they've\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the tokenizer with training data\n",
    "tokenizer = BPE(vocab_size=400, texts=training_text)\n",
    "tokenizer.learn_bpe()\n",
    "\n",
    "print(\"\\nRunning validation tests...\\n\")\n",
    "\n",
    "# Test 1: Basic English text\n",
    "print(\"Test 1: Basic English\")\n",
    "validate_bpe_results(tokenizer, \"The quick brown fox jumps over the lazy dog\", \n",
    "                    \"Basic English text\")\n",
    "\n",
    "# Test 2: Repeated words\n",
    "print(\"\\nTest 2: Repeated Words\")\n",
    "validate_bpe_results(tokenizer, \"the the the quick quick fox fox dog dog\", \n",
    "                    \"Text with repeated words\")\n",
    "\n",
    "# Test 3: Mixed case\n",
    "print(\"\\nTest 3: Mixed Case\")\n",
    "validate_bpe_results(tokenizer, \"Hello WORLD wOrLd World HELLO\", \n",
    "                    \"Mixed case handling\")\n",
    "\n",
    "# Test 4: Numbers and special characters\n",
    "print(\"\\nTest 4: Numbers and Special Characters\")\n",
    "validate_bpe_results(tokenizer, \"User123 has $100.00 in their account!\", \n",
    "                    \"Numbers and special characters\")\n",
    "\n",
    "# Test 5: Common subwords\n",
    "print(\"\\nTest 5: Subwords\")\n",
    "validate_bpe_results(tokenizer, \"playing player played plays playful playground\", \n",
    "                    \"Common subword patterns\")\n",
    "\n",
    "# Test 6: Contractions\n",
    "print(\"\\nTest 6: Contractions\")\n",
    "validate_bpe_results(tokenizer, \"I'm don't won't can't they've you're\", \n",
    "                    \"Contractions\")\n",
    "\n",
    "# Test 7: Whitespace handling\n",
    "print(\"\\nTest 7: Whitespace\")\n",
    "validate_bpe_results(tokenizer, \"Multiple    spaces   and   tabs   here\", \n",
    "                    \"Whitespace handling\")\n",
    "\n",
    "# Test 8: Unicode characters\n",
    "print(\"\\nTest 8: Unicode\")\n",
    "validate_bpe_results(tokenizer, \"cafÃ© rÃ©sumÃ© chÃ¢teau Å¡koda\", \n",
    "                    \"Unicode character handling\")\n",
    "\n",
    "# Test 9: Punctuation\n",
    "print(\"\\nTest 9: Punctuation\")\n",
    "validate_bpe_results(tokenizer, \"Hello, world! How are you? This is: amazing.\", \n",
    "                    \"Punctuation handling\")\n",
    "\n",
    "# Test 10: Long compound words\n",
    "print(\"\\nTest 10: Compound Words\")\n",
    "validate_bpe_results(tokenizer, \"tokenization implementation methodology understanding\", \n",
    "                    \"Long compound words\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artemis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
