# CSE-590 Project Playbook — Behavior Detection and Trends (Sessions 2–26)

**Scope.** Same strategy as before.  Rules → silver labels → supervised classifiers → sequence logic → trends.  Everything runs inside the course Docker/Jupyter container.  Each logic block lists the session source used.

**Sources by section.**

1. Session 2 — TextProcessing: tokenization, normalization, stemming.
2. Session 3 — Preprocessing/Feature Extraction and evaluation framing.
3. Session 4 — Sentiment Analysis with Logistic Regression.
4. Sessions 5–6 — Probability Models 1–2, Naïve Bayes.
5. Session 7 — Vector Space Model and trend framing.
6. Session 8 — Vector Transformations, K‑NN (background).
7. Session 9 — Word2Vec (background).
8. Sessions 10–13 — NN basics and cost functions (background for optional deep models).
9. Sessions 15–16 — RNNs, LSTM/GRU (optional sequence classifier).
10. Sessions 17–18 — Siamese Networks (optional similarity for rework).
11. Sessions 19–20 — Seq2Seq and Attention (background).
12. Session 21 — BLEU/ROUGE metrics (only if you add generation).
13. Session 22 — NMT with Teacher Forcing and decoding (background).
14. Sessions 23–26 — Transformers and multi‑head attention (optional encoder features).

---

## 1. Prerequisites and Environment  (Env: docker‑compose, environment.yml)

1. Open a terminal in the project folder with `docker-compose.yaml`, `environment.yml`, and your CSV.
2. Run `docker-compose up -d --build`.
3. Browse to `http://127.0.0.1:8888` and use the configured token.
4. Place `teams_messages.csv` in the mapped project folder.

Reasoning.  Container keeps versions and paths consistent for all notebooks.

---

## 2. Imports and Global Setup  (Sess2: Text Processing)

```python
import pandas as pd, numpy as np, re
from datetime import time
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import matplotlib.pyplot as plt

import nltk; nltk.download('punkt')

TZ = "America/New_York"
STEM = PorterStemmer()
TEXT_COL = "text"      # message text
TS_COL   = "timestamp" # parseable timestamp
THREAD_COL = "thread_id"
CSV_PATH = "teams_messages.csv"
```

---

## 3. Normalization Helpers  (Sess2: Tokenization, Normalization, Stemming)

```python
def normalize_rules_text(s: str) -> str:
    s = re.sub(r"https?://\S+|\S+@\S+", " ", str(s).lower())
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    return re.sub(r"\s+", " ", s).strip()


def normalize_model_text(s: str) -> str:
    toks = word_tokenize(str(s).lower())
    toks = [re.sub(r"[^a-z0-9]", "", t) for t in toks]
    toks = [t for t in toks if t]
    toks = [STEM.stem(t) for t in toks]
    return " ".join(toks)


def add_time_flags(df: pd.DataFrame, ts_col: str = TS_COL) -> pd.DataFrame:
    ts = pd.to_datetime(df[ts_col], errors="coerce")
    if ts.dt.tz is None:
        ts = ts.dt.tz_localize(TZ, nonexistent="NaT", ambiguous="NaT")
    else:
        ts = ts.dt.tz_convert(TZ)
    df = df.copy()
    df["_ts"], df["_dow"], df["_hour"] = ts, ts.dt.weekday, ts.dt.hour
    df["_is_after_5"] = df["_hour"] >= 17
    df["_is_fri_after_3"] = (df["_dow"] == 4) & (df["_hour"] >= 15)
    df["_is_weekend"] = df["_dow"].isin([5, 6])
    df["_is_after_hours"] = df["_is_after_5"] | df["_is_fri_after_3"] | df["_is_weekend"]
    return df
```

---

## 4. Load CSV and Build Base Columns  (Sess2)

```python
raw = pd.read_csv(CSV_PATH)
assert TEXT_COL in raw.columns and TS_COL in raw.columns
if THREAD_COL not in raw.columns:
    THREAD_COL = "thread_id_fallback"
    raw[THREAD_COL] = raw[TEXT_COL].str.extract(r"(#[A-Z]{1,5}-\d+)", expand=False).fillna("NA")

raw["rules_text"] = raw[TEXT_COL].apply(normalize_rules_text)
raw["model_text"] = raw[TEXT_COL].apply(normalize_model_text)
raw = add_time_flags(raw, ts_col=TS_COL)
```

---

## 5. Silver Labels: Behavior Rules  (Sess2; Sessions 5–6 framing)

```python
DELIVERY_KW = re.compile(
    r"\b(i|we)\s+(just\s+)?(approved|did|completed|finished|pushed|deployed|delivered|fixed|resolved|submitted)\b|"
    r"\b(this|it)\s+is\s+ready\b|"
    r"\bready\s+for\s+(review|deployment|prod|production|testing)\b|"
    r"\bmoved\s+to\s+(the\s+)?(correct\s+)?status\b|"
    r"\b(recipe|runbook|artifact|package)\s+is\s+ready\b|"
    r"\bmarked\s+as\s+(done|complete|resolved)\b|"
    r"\bupdated\s+the\s+(ticket|story|task)\b",
    re.I,
)
APOLOGY_KW = re.compile(r"\bsorry\b.*\blate\b|\blast\s+minute\b", re.I)
READY_KW   = re.compile(r"\bready\s+for\s+(review|test|prod|production)\b|\b(this|it)\s+is\s+ready\b", re.I)
STATUS_KW  = re.compile(r"\bmoved\s+to\s+(the\s+)?(correct\s+)?status\b|\bmarked\s+as\s+(done|complete|resolved)\b", re.I)
REWORK_KW  = re.compile(r"\b(rework|fix(ed)?\s+again|again\s+fix|redo|do\s+over|second\s+pass)\b", re.I)
RESUB_KW   = re.compile(r"\b(re-?submit(ted|ting)?|resubmission|updated\s+(pr|pull\s+request|ticket|story)|reopen(ed)?)\b", re.I)
MISSED_KW  = re.compile(r"\b(missed|forgot|overlooked|didn'?t\s+include|left\s+out|not\s+covered)\b", re.I)


def label_silver(df: pd.DataFrame) -> pd.DataFrame:
    txt = df["rules_text"]
    out = df.copy()
    out["DeliveryAfterHours"]        = (txt.str.contains(DELIVERY_KW, na=False) & out["_is_after_hours"]).astype(int)
    out["StatusChangeAfterHours"]    = (txt.str.contains(STATUS_KW,   na=False) & out["_is_after_hours"]).astype(int)
    out["ReadyForReviewAfterHours"]  = (txt.str.contains(READY_KW,    na=False) & out["_is_after_hours"]).astype(int)
    out["FridayEarlyCutoffDelivery"] = (txt.str.contains(DELIVERY_KW, na=False) & (out["_dow"]==4) & (out["_hour"]>=15)).astype(int)
    out["WeekendDelivery"]           = (txt.str.contains(DELIVERY_KW, na=False) & out["_is_weekend"]).astype(int)
    out["ApologyRush"]               =  txt.str.contains(APOLOGY_KW,  na=False).astype(int)
    out["ReworkPhrase"]              =  txt.str.contains(REWORK_KW,   na=False).astype(int)
    out["ResubmissionPhrase"]        =  txt.str.contains(RESUB_KW,    na=False).astype(int)
    out["MissedRequirementPhrase"]   =  txt.str.contains(MISSED_KW,   na=False).astype(int)
    return out

silver = label_silver(raw)
```

---

## 6. Sequence‑Aware Behavior: NotRightFirstTime  (Sess2; Sess7)

```python
seq = silver.sort_values([THREAD_COL, "_ts"]).copy()
seq["delivery_like"] = (
    seq["rules_text"].str.contains(DELIVERY_KW, na=False)
    | seq["StatusChangeAfterHours"].eq(1)
    | seq["ReadyForReviewAfterHours"].eq(1)
)
seq["_ts_unix"] = seq["_ts"].astype("int64") // 10**9
WIN_SECONDS = 72 * 3600

def prior_delivery_count(group: pd.DataFrame) -> pd.Series:
    idx, times, flags = group.index, group["_ts_unix"].values, group["delivery_like"].values
    out = np.zeros(len(group), dtype=int)
    for i in range(len(group)):
        t0 = times[i] - WIN_SECONDS
        mask = (times < times[i]) & (times >= t0) & (flags == True)
        out[i] = int(mask.sum())
    return pd.Series(out, index=idx)

seq["prior_delivery_like_72h"] = seq.groupby(THREAD_COL, group_keys=False).apply(prior_delivery_count)
seq["NotRightFirstTime"] = ((seq["delivery_like"] == True) & (seq["prior_delivery_like_72h"] >= 1)).astype(int)

silver = silver.drop(columns=[c for c in ["delivery_like", "_ts_unix", "prior_delivery_like_72h"] if c in silver.columns])
silver = silver.merge(seq[["NotRightFirstTime"]], left_index=True, right_index=True, how="left").fillna({"NotRightFirstTime":0})
silver["NotRightFirstTime"] = silver["NotRightFirstTime"].astype(int)
```

---

## 7. Supervised Classifiers  (Sess4: Logistic Regression; Sessions 5–6: Naïve Bayes)

```python
BEHAVIORS = [
    "DeliveryAfterHours","StatusChangeAfterHours","ReadyForReviewAfterHours",
    "FridayEarlyCutoffDelivery","WeekendDelivery","ApologyRush",
    "ReworkPhrase","ResubmissionPhrase","MissedRequirementPhrase","NotRightFirstTime"
]

vec = TfidfVectorizer(ngram_range=(1,2), min_df=5)
X_all = vec.fit_transform(silver["model_text"])  # Sess2 features

MODELS = {}
for b in BEHAVIORS:
    y = silver[b]
    if y.sum() == 0:
        print(f"[Skip] No positive labels for {b}.")
        continue
    Xtr, Xte, ytr, yte = train_test_split(X_all, y, test_size=0.2, random_state=42, stratify=y)

    lr = LogisticRegression(max_iter=300)  # Sess4
    lr.fit(Xtr, ytr)
    print(f"\n== {b} :: Logistic Regression ==\n", classification_report(yte, lr.predict(Xte), digits=3))

    nb = MultinomialNB()                   # Sess5–6
    nb.fit(Xtr, ytr)
    print(f"\n== {b} :: Naive Bayes ==\n", classification_report(yte, nb.predict(Xte), digits=3))

    MODELS[b] = {"vec": vec, "lr": lr, "nb": nb}
```

---

## 8. Optional Deep Sequence Classifier  (Sessions 15–16; Session 13 cost)

Purpose.  Many‑to‑One sequence model for a behavior like `NotRightFirstTime`.  Keep small and local.  Train only if time permits.

Steps.  a) Build a vocab over `rules_text` or `model_text`.  b) Pad to equal lengths.  c) Train GRU/LSTM classifier with Cross‑Entropy.  d) Compare F1 to Logistic Regression.

---

## 9. Siamese Similarity for Rework/Resubmission Pairs  (Sessions 17–18)

Purpose.  Detect near‑duplicate deliveries within a thread to support `NotRightFirstTime` and `ResubmissionPhrase` using cosine similarity of sentence embeddings.

Approach.  a) Positive pairs: same thread, close timestamps.  b) Negatives: different threads.  c) Train Siamese LSTM with cosine‑margin loss.  d) At inference, flag pairs above threshold as rework clusters.

Note.  Optional.  LR/NB remain primary.

---

## 10. Language‑Model Surprise Score  (Session 14 basics; Session 13 cost)

Purpose.  Score messages by negative log‑probability under a simple LM trained on routine updates.  High values flag anomalous late/rush contexts.

Implementation sketch.  a) Train an n‑gram LM on routine messages.  b) Compute per‑message average negative log‑probability.  c) Use a threshold as a weak feature in Logistic Regression.

---

## 11. Trend Charts  (Session 3 evaluation framing)

```python
def behavior_counts(df: pd.DataFrame, cols, freq="1H") -> pd.DataFrame:
    g = df.set_index("_ts")[cols].resample(freq).sum().fillna(0)
    return g

SELECTED = ["DeliveryAfterHours","NotRightFirstTime","ResubmissionPhrase","MissedRequirementPhrase"]
trend = behavior_counts(silver, SELECTED, "1H")

ax = trend.rolling(24, min_periods=1).mean().plot(figsize=(10,5))
ax.set_title("Rolling 24h Behavior Trends")
ax.set_ylabel("Count")
plt.show()
```

---

## 12. Near Real‑Time Batch Refresh  (Session 2 preprocessing; Session 7 trend framing)

```python
def score_behaviors(df_new: pd.DataFrame) -> pd.DataFrame:
    df_new = df_new.copy()
    df_new["rules_text"] = df_new[TEXT_COL].apply(normalize_rules_text)
    df_new["model_text"] = df_new[TEXT_COL].apply(normalize_model_text)
    df_new = add_time_flags(df_new, ts_col=TS_COL)
    df_new = label_silver(df_new)
    for b, pack in MODELS.items():
        v, lr = pack["vec"], pack["lr"]
        df_new[f"{b}_pred"] = lr.predict(v.transform(df_new["model_text"]))
    return df_new

base = silver.copy()

def refresh(csv_path: str):
    df_all = pd.read_csv(csv_path)
    if len(df_all) <= len(base):
        print("No new rows."); return None
    new = df_all.iloc[len(base):].copy()
    if THREAD_COL not in new.columns:
        new[THREAD_COL] = new[TEXT_COL].str.extract(r"(#[A-Z]{1,5}-\d+)", expand=False).fillna("NA")
    new_scored = score_behaviors(new)
    out = pd.concat([base, new_scored], ignore_index=True)
    g = behavior_counts(out, SELECTED, "1H").rolling(24, min_periods=1).mean()
    ax = g.plot(figsize=(10,5))
    ax.set_title("Rolling 24h Behavior Trends — Updated")
    ax.set_ylabel("Count")
    plt.show()
    return out
```

---

## 13. Evaluation and Metrics  (Session 3: Precision/Recall/F1; Confusion Matrix)

```python
for b in BEHAVIORS:
    if b not in MODELS:
        continue
    y = silver[b]
    X = MODELS[b]["vec"].transform(silver["model_text"])
    yhat = MODELS[b]["lr"].predict(X)
    p,r,f,_ = precision_recall_fscore_support(y, yhat, average='binary', zero_division=0)
    print(f"{b}: P={p:.3f} R={r:.3f} F1={f:.3f}")
```

If you add generation later, evaluate with BLEU/ROUGE as taught (Session 21).

---

## 14. Optional: Transformer Encoder as Feature Extractor  (Sessions 23–26)

Purpose.  Replace TF‑IDF with a transformer encoder sentence embedding computed locally, then train Logistic Regression.  Keep charts and evaluation identical.  Use only if feasible in your container.

---

## 15. Export Artifacts and Reproducibility  (syllabus + environment)

```python
silver.to_csv("processed_behaviors.csv", index=False)
silver.sample(1000, random_state=7).to_csv("sample_behaviors_1k.csv", index=False)
from joblib import dump
for b, pack in MODELS.items():
    dump(pack["lr"], f"model_lr_{b}.joblib")
    dump(pack["vec"], f"vec_{b}.joblib")
```

---

## 16. Session Map

1. Session 1 — Introduction.
2. Session 2 — Text processing, tokenization, stemming.
3. Session 3 — Preprocessing/Feature Extraction and evaluation framing.
4. Session 4 — Logistic Regression baseline.
5. Sessions 5–6 — Probability models and Naïve Bayes.
6. Session 7 — Vector space and trend framing.
7. Session 8 — Vector transformations and K‑NN (background).
8. Session 9 — Word vectors (background).
9. Sessions 10–13 — NN basics, cost functions (background).
10. Sessions 15–16 — LSTM/GRU sequence models (optional classifier).
11. Sessions 17–18 — Siamese similarity (optional rework pairing).
12. Sessions 19–20 — Seq2Seq + Attention (background).
13. Session 21 — BLEU/ROUGE (if you add generation).
14. Session 22 — Teacher Forcing/decoding (background).
15. Sessions 23–26 — Transformers and multi‑head attention (optional encoder features).

End of Markdown copy.  Paste into your repo and run inside the course container.
