{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSE-590 Project Playbook - Application of Natural Language Processing Against\n",
        "Enterprise System Chat Logs will be parsed using class provided lessons.  This is to find themes or pattern recognition needed in an enterprise system where AI is not allowed to be incororated.\n",
        "\n",
        "**Scope.** Rules > silver labels > supervised classifiers > sequence logic > trends. Everything runs inside the course Docker/Jupyter container. Each logic block lists the Sessionion source used.\n",
        "\n",
        "**Sessionion sources:** Session 2 TextProcessing; Session 3 Preprocessing/Feature Extraction; Session 4 Logistic Regression; Session 5 - 6 Probability/Naïve Bayes; Session 7 Vector Space; Session 8 - 9 Vectors background; Session 10 - 13 NN/cost background; Session 15 - 16 LSTM/GRU optional; Session 17 - 18 Siamese optional; Session19 - 20 Seq2Seq/Attention background; Session 21 BLEU/ROUGE; Session 22 Teacher Forcing; Session 23 - 26 Transformers optional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prerequisites and Environment  (Env)\n",
        "1. Open a terminal in the project folder with `docker-compose.yaml`, `environment.yml`, and your CSV.  \n",
        "2. Run `docker-compose up -d --build`.  \n",
        "3. Browse to `http://127.0.0.1:8888` and use the configured token.  \n",
        "4. Place `teams_messages.csv` in the mapped project folder.  \n",
        "\n",
        "_Container keeps versions and paths consistent for all notebooks._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and Global Setup  (Session 2: Text Processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd, numpy as np, re\n",
        "from datetime import time\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk; nltk.download('punkt')\n",
        "\n",
        "TZ = \"America/New_York\"\n",
        "STEM = PorterStemmer()\n",
        "TEXT_COL = \"text\"      # message text\n",
        "TS_COL   = \"timestamp\" # parse-able timestamp\n",
        "THREAD_COL = \"thread_id\"\n",
        "CSV_PATH = \"stackexchange_style_devops_chat_regenerated.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Normalization Helpers  (Session 2: Tokenization, Normalization, Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def normalize_rules_text(s: str) -> str:\n",
        "    s = re.sub(r\"https?://\\S+|\\S+@\\S+\", \" \", str(s).lower())\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def normalize_model_text(s: str) -> str:\n",
        "    toks = word_tokenize(str(s).lower())\n",
        "    toks = [re.sub(r\"[^a-z0-9]\", \"\", t) for t in toks]\n",
        "    toks = [t for t in toks if t]\n",
        "    toks = [STEM.stem(t) for t in toks]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "def add_time_flags(df: pd.DataFrame, ts_col: str = TS_COL) -> pd.DataFrame:\n",
        "    ts = pd.to_datetime(df[ts_col], errors=\"coerce\")\n",
        "    if ts.dt.tz is None:\n",
        "        ts = ts.dt.tz_localize(TZ, nonexistent=\"NaT\", ambiguous=\"NaT\")\n",
        "    else:\n",
        "        ts = ts.dt.tz_convert(TZ)\n",
        "    df = df.copy()\n",
        "    df[\"_ts\"], df[\"_dow\"], df[\"_hour\"] = ts, ts.dt.weekday, ts.dt.hour\n",
        "    df[\"_is_after_5\"] = df[\"_hour\"] >= 17\n",
        "    df[\"_is_fri_after_3\"] = (df[\"_dow\"] == 4) & (df[\"_hour\"] >= 15)\n",
        "    df[\"_is_weekend\"] = df[\"_dow\"].isin([5, 6])\n",
        "    df[\"_is_after_hours\"] = df[\"_is_after_5\"] | df[\"_is_fri_after_3\"] | df[\"_is_weekend\"]\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load CSV and Build Base Columns  (Session 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEXT_COL = \"text\"\n",
        "TS_COL = \"created_at\"\n",
        "\n",
        "raw = pd.read_csv(CSV_PATH).rename(columns={\n",
        "    \"content\": TEXT_COL,\n",
        "    \"timestamp\": TS_COL\n",
        "})[[TEXT_COL, TS_COL]]\n",
        "\n",
        "assert TEXT_COL in raw.columns and TS_COL in raw.columns\n",
        "\n",
        "if THREAD_COL not in raw.columns:\n",
        "    THREAD_COL = \"thread_id_fallback\"\n",
        "    raw[THREAD_COL] = raw[TEXT_COL].str.extract(r\"(#[A-Z]{1,5}-\\d+)\", expand=False).fillna(\"NA\")\n",
        "\n",
        "raw[\"rules_text\"] = raw[TEXT_COL].apply(normalize_rules_text)\n",
        "raw[\"model_text\"] = raw[TEXT_COL].apply(normalize_model_text)\n",
        "raw = add_time_flags(raw, ts_col=TS_COL)\n",
        "\n",
        "raw.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Silver Labels: Behavior Rules  (Session 2; Session 5 – 6 framing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "DELIVERY_KW = re.compile(\n",
        "    r\"\\b(?:i|we)\\s+(?:just\\s+)?(?:approved|did|completed|finished|pushed|deployed|delivered|fixed|resolved|submitted)\\b|\"\n",
        "    r\"\\b(?:this|it)\\s+is\\s+ready\\b|\"\n",
        "    r\"\\bready\\s+for\\s+(?:review|deployment|prod|production|testing)\\b|\"\n",
        "    r\"\\bmoved\\s+to\\s+(?:the\\s+)?(?:correct\\s+)?status\\b|\"\n",
        "    r\"\\b(?:recipe|runbook|artifact|package)\\s+is\\s+ready\\b|\"\n",
        "    r\"\\bmarked\\s+as\\s+(?:done|complete|resolved)\\b|\"\n",
        "    r\"\\bupdated\\s+the\\s+(?:ticket|story|task)\\b\",\n",
        "    re.I,\n",
        ")\n",
        "\n",
        "APOLOGY_KW = re.compile(r\"\\bsorry\\b.*\\blate\\b|\\blast\\s+minute\\b\", re.I)\n",
        "READY_KW = re.compile(r\"\\bready\\s+for\\s+(?:review|test|prod|production)\\b|\\b(?:this|it)\\s+is\\s+ready\\b\", re.I)\n",
        "STATUS_KW = re.compile(r\"\\bmoved\\s+to\\s+(?:the\\s+)?(?:correct\\s+)?status\\b|\\bmarked\\s+as\\s+(?:done|complete|resolved)\\b\", re.I)\n",
        "REWORK_KW = re.compile(r\"\\b(?:rework|fix(?:ed)?\\s+again|again\\s+fix|redo|do\\s+over|second\\s+pass)\\b\", re.I)\n",
        "RESUB_KW = re.compile(r\"\\b(?:re-?submit(?:ted|ting)?|resubmission|updated\\s+(?:pr|pull\\s+request|ticket|story)|reopen(?:ed)?)\\b\", re.I)\n",
        "MISSED_KW = re.compile(r\"\\b(?:missed|forgot|overlooked|didn'?t\\s+include|left\\s+out|not\\s+covered)\\b\", re.I)\n",
        "\n",
        "def label_silver(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    # Always ensure text is string (avoids errors on NaN)\n",
        "    txt = out[\"rules_text\"].astype(str)\n",
        "\n",
        "    out[\"DeliveryAfterHours\"]        = (txt.str.contains(DELIVERY_KW, na=False) & out[\"_is_after_hours\"]).astype(int)\n",
        "    out[\"StatusChangeAfterHours\"]    = (txt.str.contains(STATUS_KW,   na=False) & out[\"_is_after_hours\"]).astype(int)\n",
        "    out[\"ReadyForReviewAfterHours\"]  = (txt.str.contains(READY_KW,    na=False) & out[\"_is_after_hours\"]).astype(int)\n",
        "    out[\"FridayEarlyCutoffDelivery\"] = (txt.str.contains(DELIVERY_KW, na=False) & (out[\"_dow\"] == 4) & (out[\"_hour\"] >= 15)).astype(int)\n",
        "    out[\"WeekendDelivery\"]           = (txt.str.contains(DELIVERY_KW, na=False) & out[\"_is_weekend\"]).astype(int)\n",
        "\n",
        "    out[\"ApologyRush\"]               = txt.str.contains(APOLOGY_KW, na=False).astype(int)\n",
        "    out[\"ReworkPhrase\"]              = txt.str.contains(REWORK_KW,  na=False).astype(int)\n",
        "    out[\"ResubmissionPhrase\"]        = txt.str.contains(RESUB_KW,   na=False).astype(int)\n",
        "    out[\"MissedRequirementPhrase\"]   = txt.str.contains(MISSED_KW,  na=False).astype(int)\n",
        "\n",
        "    return out\n",
        "\n",
        "silver = label_silver(raw)\n",
        "silver.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sequence‑Aware Behavior: NotRightFirstTime  (Session 2; Session 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seq = silver.sort_values([THREAD_COL, \"_ts\"]).copy()\n",
        "\n",
        "seq[\"delivery_like\"] = (\n",
        "    seq[\"rules_text\"].str.contains(DELIVERY_KW, na=False)\n",
        "    | seq[\"StatusChangeAfterHours\"].eq(1)\n",
        "    | seq[\"ReadyForReviewAfterHours\"].eq(1)\n",
        ")\n",
        "\n",
        "# convert timestamps\n",
        "seq[\"_ts_unix\"] = seq[\"_ts\"].astype(\"int64\") // 10**9\n",
        "WIN_SECONDS = 72 * 3600\n",
        "\n",
        "# initialize result array\n",
        "prior_counts = np.zeros(len(seq), dtype=int)\n",
        "\n",
        "# compute per-thread rolling window counts\n",
        "for thread, sub in seq.groupby(THREAD_COL):\n",
        "    idx = sub.index\n",
        "    ts = sub[\"_ts_unix\"].values\n",
        "    flags = sub[\"delivery_like\"].values\n",
        "\n",
        "    # sliding two-pointer window\n",
        "    left = 0\n",
        "    for right in range(len(sub)):\n",
        "        while ts[right] - ts[left] > WIN_SECONDS:\n",
        "            left += 1\n",
        "        # count TRUEs in window excluding current\n",
        "        prior_counts[idx[right]] = flags[left:right].sum()\n",
        "\n",
        "seq[\"prior_delivery_like_72h\"] = prior_counts\n",
        "\n",
        "seq[\"NotRightFirstTime\"] = (\n",
        "    seq[\"delivery_like\"] & (seq[\"prior_delivery_like_72h\"] >= 1)\n",
        ").astype(int)\n",
        "\n",
        "# merge back\n",
        "silver = silver.merge(\n",
        "    seq[[\"NotRightFirstTime\"]],\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        "    how=\"left\"\n",
        ").fillna({\"NotRightFirstTime\": 0})\n",
        "\n",
        "silver[\"NotRightFirstTime\"] = silver[\"NotRightFirstTime\"].astype(int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Supervised Classifiers  (Session 4; Session 5 – 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "BEHAVIORS = [\n",
        "    \"DeliveryAfterHours\",\"StatusChangeAfterHours\",\"ReadyForReviewAfterHours\",\n",
        "    \"FridayEarlyCutoffDelivery\",\"WeekendDelivery\",\"ApologyRush\",\n",
        "    \"ReworkPhrase\",\"ResubmissionPhrase\",\"MissedRequirementPhrase\",\"NotRightFirstTime\"\n",
        "]\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
        "X_all = vec.fit_transform(silver[\"model_text\"])\n",
        "\n",
        "MODELS = {}\n",
        "for b in BEHAVIORS:\n",
        "    y = silver[b]\n",
        "    if y.sum() == 0:\n",
        "        print(f\"[Skip] No positive labels for {b}.\")\n",
        "        continue\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X_all, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    lr = LogisticRegression(max_iter=300)\n",
        "    lr.fit(Xtr, ytr)\n",
        "    print(f\"\\n== {b} :: Logistic Regression ==\\n\", classification_report(yte, lr.predict(Xte), digits=3))\n",
        "\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(Xtr, ytr)\n",
        "    print(f\"\\n== {b} :: Naive Bayes ==\\n\", classification_report(yte, nb.predict(Xte), digits=3))\n",
        "\n",
        "    MODELS[b] = {\"vec\": vec, \"lr\": lr, \"nb\": nb}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Trend Charts  (Session 3 evaluation framing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def behavior_counts(df: pd.DataFrame, cols, freq=\"1H\") -> pd.DataFrame:\n",
        "    g = df.set_index(\"_ts\")[cols].resample(freq).sum().fillna(0)\n",
        "    return g\n",
        "\n",
        "SELECTED = [\"DeliveryAfterHours\",\"NotRightFirstTime\",\"ResubmissionPhrase\",\"MissedRequirementPhrase\"]\n",
        "trend = behavior_counts(silver, SELECTED, \"1H\")\n",
        "\n",
        "ax = trend.rolling(24, min_periods=1).mean().plot(figsize=(10,5))\n",
        "ax.set_title(\"Rolling 24h Behavior Trends\")\n",
        "ax.set_ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Near Real ‑ Time Batch Refresh  (Session 2; Session 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b2157f8",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def score_behaviors(df_new: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_new = df_new.copy()\n",
        "    df_new[\"rules_text\"] = df_new[TEXT_COL].apply(normalize_rules_text)\n",
        "    df_new[\"model_text\"] = df_new[TEXT_COL].apply(normalize_model_text)\n",
        "    df_new = add_time_flags(df_new, ts_col=TS_COL)\n",
        "    df_new = label_silver(df_new)\n",
        "    for b, pack in MODELS.items():\n",
        "        v, lr = pack[\"vec\"], pack[\"lr\"]\n",
        "        df_new[f\"{b}_pred\"] = lr.predict(v.transform(df_new[\"model_text\"]))\n",
        "    return df_new\n",
        "\n",
        "base = silver.copy()\n",
        "\n",
        "def refresh(csv_path: str):\n",
        "    df_all = pd.read_csv(csv_path)\n",
        "    if len(df_all) <= len(base):\n",
        "        print(\"No new rows.\"); return None\n",
        "    new = df_all.iloc[len(base):].copy()\n",
        "    if THREAD_COL not in new.columns:\n",
        "        new[THREAD_COL] = new[TEXT_COL].str.extract(r\"(#[A-Z]{1,5}-\\d+)\", expand=False).fillna(\"NA\")\n",
        "    new_scored = score_behaviors(new)\n",
        "    out = pd.concat([base, new_scored], ignore_index=True)\n",
        "    g = behavior_counts(out, SELECTED, \"1H\").rolling(24, min_periods=1).mean()\n",
        "    ax = g.plot(figsize=(10,5))\n",
        "    ax.set_title(\"Rolling 24h Behavior Trends — Updated\")\n",
        "    ax.set_ylabel(\"Count\")\n",
        "    plt.show()\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Sentiment  (Session 4 – 6 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "POS_KW = re.compile(r\"\\b(?:thanks|great|appreciate|nice|good)\\b\", re.I)\n",
        "NEG_KW = re.compile(r\"\\b(?:bad|frustrat|angry|upset|issue|broken)\\b\", re.I)\n",
        "\n",
        "sent = silver.copy()\n",
        "sent[\"sent_rule\"] = 0\n",
        "sent.loc[sent[\"rules_text\"].str.contains(POS_KW, na=False), \"sent_rule\"] = 1\n",
        "sent.loc[sent[\"rules_text\"].str.contains(NEG_KW, na=False), \"sent_rule\"] = -1\n",
        "\n",
        "# Re-use X_all from TF-IDF of model_text\n",
        "Xtr, Xte, ytr, yte = train_test_split(X_all, sent[\"sent_rule\"], test_size=0.2, random_state=42, stratify=sent[\"sent_rule\"])\n",
        "logreg_sent = LogisticRegression(max_iter=300)\n",
        "logreg_sent.fit(Xtr, ytr)\n",
        "print(classification_report(yte, logreg_sent.predict(Xte), digits=3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Export Artifacts  (Reproducibility)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "silver.to_csv(\"processed_behaviors.csv\", index=False)\n",
        "silver.sample(1000, random_state=7).to_csv(\"sample_behaviors_1k.csv\", index=False)\n",
        "from joblib import dump\n",
        "for b, pack in MODELS.items():\n",
        "    dump(pack[\"lr\"], f\"model_lr_{b}.joblib\")\n",
        "    dump(pack[\"vec\"], f\"vec_{b}.joblib\")\n",
        "print(\"Artifacts saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
